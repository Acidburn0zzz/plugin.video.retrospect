# coding=utf-8
#==============================================================================
# LICENSE Retrospect-Framework - CC BY-NC-ND
#===============================================================================
# This work is licenced under the Creative Commons
# Attribution-Non-Commercial-No Derivative Works 3.0 Unported License. To view a
# copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/3.0/
# or send a letter to Creative Commons, 171 Second Street, Suite 300,
# San Francisco, California 94105, USA.
#===============================================================================

import urlparse
from datetime import datetime

from mediaitem import MediaItem, MediaItemPart
from locker import LockWithDialog

from regexer import Regexer
from cloaker import Cloaker
from xbmcwrapper import XbmcWrapper, XbmcDialogProgressWrapper
from config import Config
from initializer import Initializer
from logger import Logger
from urihandler import UriHandler
from parserdata import ParserData
from textures import TextureHandler

from helpers.htmlentityhelper import HtmlEntityHelper
from helpers.encodinghelper import EncodingHelper
from helpers.jsonhelper import JsonHelper
from helpers.languagehelper import LanguageHelper
from helpers.statistics import Statistics
from addonsettings import AddonSettings, LOCAL


class Channel:
    """
    main class from which all channels inherit
    """

    def __init__(self, channel_info):
        """Initialisation of the class.

        Arguments:
        channel_info: ChannelInfo - The channel info object to base this channel on.

        All class variables should be instantiated here and this method should not
        be overridden by any derived classes.

        """

        Logger.info("Initializing channel (__init__): %s", channel_info)

        self.mainListItems = []
        self.parentItem = None

        # The proxy to be used for this channel
        self.proxy = AddonSettings.get_proxy_for_channel(channel_info)
        self.localIP = AddonSettings.get_local_ip_header_for_channel(channel_info)

        # More and more API's need a specific set of headers. This set is used for the self.mainListUri, and is set to
        # all items generated by the chn_class.py.
        self.httpHeaders = dict()
        self.loggedOn = False

        # Initialize channel stuff from ChannelInfo object
        self.guid = channel_info.guid
        self.id = channel_info.id

        self.channelName = channel_info.channelName
        self.safeName = channel_info.safe_name
        self.channelCode = channel_info.channelCode
        self.channelDescription = channel_info.channelDescription
        self.moduleName = channel_info.moduleName
        self.compatiblePlatforms = channel_info.compatiblePlatforms
        self.sortOrder = channel_info.sortOrder
        self.category = channel_info.category
        self.language = channel_info.language
        self.path = channel_info.path
        self.version = channel_info.version

        # get the textures from the channelinfo and get their full uri's.
        self.icon = TextureHandler.instance().get_texture_uri(self, channel_info.icon)
        self.fanart = TextureHandler.instance().get_texture_uri(self, channel_info.fanart)

        # ============== Actual channel setup STARTS here and should be overwritten from derived classes ===============
        self.noImage = ""

        # set context menu items
        self.contextMenuItems = []

        # configure login stuff
        self.requiresLogon = False

        # setup the urls
        self.mainListUri = ""
        self.baseUrl = ""
        self.swfUrl = ""

        # setup the main parsing data
        # self.dataHandlers = dict()
        # self.updateHandlers = dict()
        self.dataParsers = dict()

        self.episodeItemRegex = ''      # : used for the ParseMainList
        self.episodeItemJson = None     # : used for the ParseMainList
        self.videoItemRegex = ''        # : used for the ParseMainList
        self.videoItemJson = None       # : used for the ParseMainList
        self.folderItemRegex = ''       # : used for the CreateFolderItem
        self.folderItemJson = None      # : used for the CreateFolderItem
        self.mediaUrlRegex = ''         # : used for the UpdateVideoItem
        self.mediaUrlJson = None        # : used for the UpdateVideoItem

        """
            The ProcessPageNavigation method will parse the current data using the pageNavigationRegex. It will
            create a pageItem using the CreatePageItem method. If no CreatePageItem method is in the channel,
            a default one will be created with the number present in the resultset location specified in the
            pageNavigationRegexIndex and the url from the combined resultset. If that url does not contain http://
            the self.baseUrl will be added.
        """
        self.pageNavigationIndicationRegex = ''
        self.pageNavigationRegex = ''
        self.pageNavigationJson = None
        self.pageNavigationRegexIndex = 0
        self.pageNavigationJsonIndex = None

        #===============================================================================================================
        # non standard items

        #===============================================================================================================
        # Test cases:

        # ====================================== Actual channel setup STOPS here =======================================
        return

    def init_channel(self):
        """Initializes the channel and will call some post processing stuff.

        This method is called for each add-on call and can be used to do some
        channel initialisation.

        """

        Logger.debug("Initializing channel (init_channel): %s", self)

        # Make sure all images are from the correct absolute location
        # self.icon = self.GetImageLocation(self.icon) -> already in the __init__
        # self.fanart = self.GetImageLocation(self.fanart) -> already in the __init__
        self.noImage = TextureHandler.instance().get_texture_uri(self, self.noImage)
        return

    def process_folder_list(self, item=None):
        """Process the selected item and get's it's child items using the available dataparsers.

        Accepts an <item> and returns a list of MediaListems with at least name & url
        set. The following actions are done:

        * determining the correct parsers to use
        * call a pre-processor
        * parsing the data with the parsers
        * calling the creators for item creations

        if the item is NOne, we assume that we are dealing with the first call for this channel and the mainlist uri
        is used.

        :param MediaItem|none item: The parent item.

        :return: A list of MediaItems that form the childeren of the <item>.
        :rtype: list[MediaItem]

        """

        items = []
        self.parentItem = item

        if item is None:
            Logger.info("process_folder_list :: No item was specified. Assuming it was the main channel list")
            url = self.mainListUri
        elif len(item.items) > 0:
            return item.items
        else:
            url = item.url

        # Determine the handlers and process
        data_parsers = self.__GetDataParsers(url)
        # Exclude the updaters only
        data_parsers = filter(lambda p: not p.is_video_updater_only(), data_parsers)
        if filter(lambda p: p.LogOnRequired, data_parsers):
            Logger.info("One or more dataparsers require logging in.")
            self.loggedOn = self.LogOn()

        # now set the headers here and not earlier in case they might have been update by the logon
        if item is not None and item.HttpHeaders:
            headers = item.HttpHeaders
        else:
            headers = self.httpHeaders

        # Let's retrieve the required data. Main url's
        if url.startswith("http:") or url.startswith("https:") or url.startswith("file:"):
            # Disable cache on live folders
            no_cache = item is not None and not item.is_playable() and item.isLive
            if no_cache:
                Logger.debug("Disabling cache for '%s'", item)
            data = UriHandler.open(url, proxy=self.proxy, additional_headers=headers, no_cache=no_cache)
        # Searching a site using search_site()
        elif url == "searchSite" or url == "#searchSite":
            Logger.debug("Starting to search")
            return self.search_site()
        # Labels instead of url's
        elif url.startswith("#"):
            data = ""
        # Others
        else:
            Logger.debug("Unknown URL format. Setting data to ''")
            data = ""

        # first check if there is a generic pre-processor
        pre_procs = filter(lambda p: p.is_generic_pre_processor(), data_parsers)
        num_pre_procs = len(pre_procs)
        Logger.trace("Processing %s Generic Pre-Processors DataParsers", num_pre_procs)
        if num_pre_procs > 1:
            # warn for strange results if more than 1 generic pre-processor is present.
            Logger.warning("More than one Generic Pre-Processor is found (%s). They are being processed in the "
                           "order that Python likes which might result in unexpected result.", num_pre_procs)

        for data_parser in pre_procs:
            # remove it from the list
            data_parsers.remove(data_parser)

            # and process it
            Logger.debug("Processing %s", data_parser)
            (data, pre_items) = data_parser.PreProcessor(data)
            items += pre_items

            if isinstance(data, JsonHelper):
                Logger.debug("Generic preprocessor resulted in JsonHelper data")

        # The the other handlers
        Logger.trace("Processing %s Normal DataParsers", len(data_parsers))
        handler_json = None
        for data_parser in data_parsers:
            Logger.debug("Processing %s", data_parser)

            # Check for preprocessors
            if data_parser.PreProcessor:
                Logger.debug("Processing DataParser.PreProcessor")
                (handler_data, pre_items) = data_parser.PreProcessor(data)
                items += pre_items
            else:
                handler_data = data

            Logger.debug("Processing DataParser.Parser")
            if data_parser.Parser is None or (data_parser.Parser == "" and not data_parser.IsJson):
                if data_parser.Creator:
                    Logger.warning("No <parser> found for %s. Skipping.", data_parser.Creator)
                continue

            if data_parser.IsJson:
                if handler_json is None:
                    # Cache the json requests to improve performance
                    Logger.trace("Caching JSON results for Dataparsing")
                    if isinstance(handler_data, JsonHelper):
                        handler_json = handler_data
                    else:
                        handler_json = JsonHelper(handler_data, Logger.instance())

                Logger.trace(data_parser.Parser)
                parser_results = handler_json.get_value(fallback=[], *data_parser.Parser)

                if not isinstance(parser_results, (tuple, list)):
                    # if there is just one match, return that as a list
                    parser_results = [parser_results]
            else:
                if isinstance(handler_data, JsonHelper):
                    raise ValueError("Cannot perform Regex Parser on JsonHelper.")
                else:
                    parser_results = Regexer.do_regex(data_parser.Parser, handler_data)

            Logger.debug("Processing DataParser.Creator for %s items", len(parser_results))
            for parser_result in parser_results:
                handler_result = data_parser.Creator(parser_result)
                if handler_result is not None:
                    if isinstance(handler_result, list):
                        items += handler_result
                    else:
                        items.append(handler_result)

        # should we exclude DRM/GEO?
        hide_geo_locked = AddonSettings.hide_geo_locked_items_for_location(self.language)
        hide_drm_protected = AddonSettings.hide_drm_items()
        hide_premium = AddonSettings.hide_premium_items()
        hide_folders = AddonSettings.hide_restricted_folders()
        type_to_exclude = None
        if not hide_folders:
            type_to_exclude = "folder"

        old_count = len(items)
        if hide_drm_protected:
            Logger.debug("Hiding DRM items")
            items = filter(lambda i: not i.isDrmProtected or i.type == type_to_exclude, items)
        if hide_geo_locked:
            Logger.debug("Hiding GEO Locked items due to GEO region: %s", self.language)
            items = filter(lambda i: not i.isGeoLocked or i.type == type_to_exclude, items)
        if hide_premium:
            Logger.debug("Hiding Premium items")
            items = filter(lambda i: not i.isPaid or i.type == type_to_exclude, items)

        cloaker = Cloaker(self, AddonSettings.store(LOCAL), logger=Logger.instance())
        if not AddonSettings.show_cloaked_items():
            Logger.debug("Hiding Cloaked items")
            items = filter(lambda i: not cloaker.is_cloaked(i.url), items)
        else:
            cloaked_items = filter(lambda i: cloaker.is_cloaked(i.url), items)
            for c in cloaked_items:
                c.isCloaked = True

        if len(items) != old_count:
            Logger.info("Hidden %s items due to DRM/GEO/Premium/cloak filter (Hide Folders=%s)",
                        old_count - len(items), hide_folders)

        # Check for grouping or not
        limit = AddonSettings.get_list_limit()
        folder_items = filter(lambda x: x.type.lower() == "folder", items)

        # we should also de-duplicate before calculating
        folder_items = list(set(folder_items))
        folders = len(folder_items)

        if 0 < limit < folders:
            # let's filter them by alphabet if the number is exceeded
            Logger.debug("Creating Groups for list exceeding '%s' folder items. Total folders found '%s'.",
                         limit, folders)
            other = LanguageHelper.get_localized_string(LanguageHelper.OtherChars)
            title_format = LanguageHelper.get_localized_string(LanguageHelper.StartWith)
            result = dict()
            non_grouped = []
            # Should we remove prefixes just as Kodi does?
            # prefixes = ("de", "het", "the", "een", "a", "an")

            for sub_item in items:
                if sub_item.dontGroup or sub_item.type != "folder":
                    non_grouped.append(sub_item)
                    continue

                char = sub_item.name[0].upper()
                # Should we de-prefix?
                # for p in prefixes:
                #     if sub_item.name.lower().startswith(p + " "):
                #         char = sub_item.name[len(p) + 1][0].upper()

                if char.isdigit():
                    char = "0-9"
                elif not char.isalpha():
                    char = other

                if char not in result:
                    Logger.trace("Creating Grouped item from: %s", sub_item)
                    if char == other:
                        item = MediaItem(title_format.replace("'", "") % (char,), "")
                    else:
                        item = MediaItem(title_format % (char.upper(),), "")
                    item.thumb = self.noImage
                    item.complete = True
                    # item.set_date(2100 + ord(char[0]), 1, 1, text='')
                    result[char] = item
                else:
                    item = result[char]
                item.items.append(sub_item)

            items = non_grouped + result.values()

        Logger.trace("Found '%s' items", len(items))
        return list(set(items))

    def process_video_item(self, item):
        """ Process a video item using the required dataparsers

        :param MediaItem item:    The Item to update

        :return: An updated item.
        :rtype: MediaItem

        """

        data_parsers = self.__GetDataParsers(item.url)
        if not data_parsers:
            Logger.error("No dataparsers found cannot update item.")
            return item

        data_parsers = filter(lambda d: d.Updater is not None, data_parsers)
        if len(data_parsers) < 1:
            Logger.warning("No DataParsers with Updaters found.")
            return item

        if len(data_parsers) > 1:
            Logger.warning("More than 2 DataParsers with Updaters found. Only using first one.")
        data_parser = data_parsers[0]

        if not data_parser.Updater:
            Logger.error("No videoupdater found cannot update item.")
            return item

        if data_parser.LogOnRequired:
            Logger.info("One or more dataparsers require logging in.")
            self.loggedOn = self.LogOn()

        Logger.debug("Processing Updater from %s", data_parser)
        return data_parser.Updater(item)

    def search_site(self, url=None):
        """ Creates an list of items by searching the site.

        This method is called when the URL of an item is "searchSite". The channel
        calling this should implement the search functionality. This could also include
        showing of an input keyboard and following actions.

        The %s the url will be replaced with an URL encoded representation of the
        text to search for.

        :param str url:     Url to use to search with a %s for the search parameters.

        :return: A list with search results as MediaItems.
        :rtype: list[MediaItem]

        """

        items = []
        if url is None:
            item = MediaItem("Search Not Implented", "", type='video')
            item.icon = self.icon
            items.append(item)
        else:
            items = []
            needle = XbmcWrapper.show_key_board()
            if needle:
                Logger.debug("Searching for '%s'", needle)
                # convert to HTML
                needle = HtmlEntityHelper.url_encode(needle)
                search_url = url % (needle, )
                temp = MediaItem("Search", search_url)
                return self.process_folder_list(temp)

        return items

    def CreateEpisodeItem(self, resultSet):
        """Creates a new MediaItem for an episode

        Arguments:
        resultSet : list[string] - the resultSet of the self.episodeItemRegex

        Returns:
        A new MediaItem of type 'folder'

        This method creates a new MediaItem from the Regular Expression or Json
        results <resultSet>. The method should be implemented by derived classes
        and are specific to the channel.

        """

        Logger.trace(resultSet)

        # Validate the input and raise errors
        if not isinstance(resultSet, dict):
            Logger.critical("No Dictionary as a resultSet. Implement a custom CreateEpisodeItem")
            raise NotImplementedError("No Dictionary as a resultSet. Implement a custom CreateEpisodeItem")

        elif "title" not in resultSet or "url" not in resultSet:
            Logger.warning("No ?P<title> or ?P<url> in resultSet")
            raise LookupError("No ?P<title> or ?P<url> in resultSet")

        # the URL
        url = resultSet["url"]
        if not url.startswith("http"):
            url = "%s/%s" % (self.baseUrl.rstrip('/'), url.lstrip('/'))

        # the title
        title = resultSet["title"]
        if title.isupper():
            title = title.title()

        item = MediaItem(title, url)
        item.thumb = resultSet.get("thumburl", None)
        item.description = resultSet.get("description", "")
        item.icon = self.icon
        item.complete = True
        item.fanart = self.fanart
        item.HttpHeaders = self.httpHeaders
        return item

    def PreProcessFolderList(self, data):
        """Performs pre-process actions for data processing

        Arguments:
        data : string - the retrieve data that was loaded for the current item and URL.

        Returns:
        A tuple of the data and a list of MediaItems that were generated.


        Accepts an data from the process_folder_list method, BEFORE the items are
        processed. Allows setting of parameters (like title etc) for the channel.
        Inside this method the <data> could be changed and additional items can
        be created.

        The return values should always be instantiated in at least ("", []).

        """

        Logger.info("Performing Pre-Processing")
        items = []
        Logger.debug("Pre-Processing finished")
        return data, items

    # def ProcessPageNavigation(self, data):
    #     """Generates a list of pageNavigation items.
    #
    #     Arguments:
    #     data : string - the retrieve data that was loaded for the current item and URL.
    #
    #     Returns:
    #     A list of MediaItems of type 'page'
    #
    #     Parses the <data> using the self.pageNavigationRegex and then calls the
    #     self.CreatePageItem method for each result to create a page item. The
    #     list of those items is returned.
    #
    #     """
    #
    #     Logger.Debug("Starting ProcessPageNavigation")
    #
    #     pageItems = []
    #     pages = []
    #
    #     # try the regex on the current data
    #     if not self.pageNavigationRegex == "" and not self.pageNavigationRegex is None:
    #         pages = Regexer.do_regex(self.pageNavigationRegex, data)
    #
    #     elif not self.pageNavigationJson is None:
    #         pageJson = JsonHelper(data, logger=Logger.instance())
    #         pages = pageJson.get_value(*self.pageNavigationJson)
    #
    #         if pages is None:
    #             # no matches, so no pages
    #             pages = []
    #         elif not isinstance(pages, (tuple, list)):
    #             # if there is just one match, return that as a list
    #             pages = [pages]
    #
    #     if len(pages) == 0:
    #         Logger.Debug("No pages found.")
    #         return pageItems
    #
    #     Logger.Debug('Starting CreatePageItem for %s items', len(pages))
    #     for page in pages:
    #         Logger.Trace('Starting CreatePageItem for %s', self.channelName)
    #         item = self.CreatePageItem(page)
    #         if not item is None:
    #             pageItems.append(item)
    #
    #     # Filter out the duplicates using the HASH power of a set
    #     pageItems = list(set(pageItems))
    #
    #     # Logger.Debug(pageItems)
    #     return pageItems

    def CreatePageItem(self, resultSet):
        """Creates a MediaItem of type 'page' using the resultSet from the regex.

        Arguments:
        resultSet : tuple(string) - the resultSet of the self.pageNavigationRegex

        Returns:
        A new MediaItem of type 'page'

        This method creates a new MediaItem from the Regular Expression or Json
        results <resultSet>. The method should be implemented by derived classes
        and are specific to the channel.

        """

        Logger.debug("Starting CreatePageItem")
        total = ''

        for result in resultSet:
            total = "%s%s" % (total, result)

        total = HtmlEntityHelper.strip_amp(total)

        if not self.pageNavigationRegexIndex == '':
            item = MediaItem(resultSet[self.pageNavigationRegexIndex], urlparse.urljoin(self.baseUrl, total))
        else:
            item = MediaItem("0", "")

        item.type = "page"
        item.fanart = self.fanart
        item.HttpHeaders = self.httpHeaders

        Logger.debug("Created '%s' for url %s", item.name, item.url)
        return item

    def CreateFolderItem(self, resultSet):
        """Creates a MediaItem of type 'folder' using the resultSet from the regex.

        Arguments:
        resultSet : tuple(strig) - the resultSet of the self.folderItemRegex

        Returns:
        A new MediaItem of type 'folder'

        This method creates a new MediaItem from the Regular Expression or Json
        results <resultSet>. The method should be implemented by derived classes
        and are specific to the channel.

        """

        Logger.trace(resultSet)

        # Validate the input and raise errors
        if not isinstance(resultSet, dict):
            Logger.critical("No Dictionary as a resultSet. Implement a custom CreateVideoItem")
            raise NotImplementedError("No Dictionary as a resultSet. Implement a custom CreateVideoItem")

        elif "title" not in resultSet or "url" not in resultSet:
            Logger.warning("No ?P<title> or ?P<url> in resultSet")
            raise LookupError("No ?P<title> or ?P<url> in resultSet")

        # The URL
        url = resultSet["url"]
        if not url.startswith("http"):
            url = "%s/%s" % (self.baseUrl.rstrip('/'), url.lstrip('/'))

        # The title
        title = resultSet["title"]
        if title.isupper():
            title = title.title()

        item = MediaItem(title, url)
        item.description = resultSet.get("description", "")
        item.thumb = resultSet.get("thumburl", "")
        item.icon = self.icon
        item.type = 'folder'
        item.fanart = self.fanart
        item.HttpHeaders = self.httpHeaders
        item.complete = True
        return item

    def CreateVideoItem(self, resultSet):
        """Creates a MediaItem of type 'video' using the resultSet from the regex.

        Arguments:
        resultSet : tuple (string) - the resultSet of the self.videoItemRegex

        Returns:
        A new MediaItem of type 'video' or 'audio' (despite the method's name)

        This method creates a new MediaItem from the Regular Expression or Json
        results <resultSet>. The method should be implemented by derived classes
        and are specific to the channel.

        If the item is completely processed an no further data needs to be fetched
        the self.complete property should be set to True. If not set to True, the
        self.UpdateVideoItem method is called if the item is focussed or selected
        for playback.

        """

        Logger.trace(resultSet)

        # Validate the input and raise errors
        if not isinstance(resultSet, dict):
            Logger.critical("No Dictionary as a resultSet. Implement a custom CreateVideoItem")
            raise NotImplementedError("No Dictionary as a resultSet. Implement a custom CreateVideoItem")

        elif "title" not in resultSet or "url" not in resultSet:
            Logger.warning("No ?P<title> or ?P<url> in resultSet")
            raise LookupError("No ?P<title> or ?P<url> in resultSet")

        # The URL
        url = resultSet["url"]
        if not url.startswith("http"):
            url = "%s/%s" % (self.baseUrl.rstrip('/'), url.lstrip('/'))

        # The title
        if "subtitle" in resultSet and resultSet["subtitle"]:
            title = "%(title)s - %(subtitle)s" % resultSet
        else:
            title = resultSet["title"]
        if title.isupper():
            title = title.title()

        item = MediaItem(title, url)
        item.description = resultSet.get("description", "")
        item.thumb = resultSet.get("thumburl", "")
        if item.thumb and not item.thumb.startswith("http"):
            item.thumb = "{}{}".format(self.baseUrl.rstrip('/'), item.thumb)

        item.icon = self.icon
        item.type = 'video'
        item.fanart = self.fanart
        item.HttpHeaders = self.httpHeaders
        item.complete = False
        return item

    def UpdateVideoItem(self, item):
        """Updates an existing MediaItem with more data.

        Arguments:
        item : MediaItem - the MediaItem that needs to be updated

        Returns:
        The original item with more data added to it's properties.

        Used to update none complete MediaItems (self.complete = False). This
        could include opening the item's URL to fetch more data and then process that
        data or retrieve it's real media-URL.

        The method should at least:
        * cache the thumbnail to disk (use self.noImage if no thumb is available).
        * set at least one MediaItemPart with a single MediaStream.
        * set self.complete = True.

        if the returned item does not have a MediaItemPart then the self.complete flag
        will automatically be set back to False.

        """

        Logger.debug('Starting UpdateVideoItem for %s (%s)', item.name, self.channelName)

        data = UriHandler.open(item.url, proxy=self.proxy, additional_headers=item.HttpHeaders)

        url = Regexer.do_regex(self.mediaUrlRegex, data)[-1]
        part = MediaItemPart(item.name, url)
        item.MediaItemParts.append(part)

        Logger.info('finishing UpdateVideoItem. MediaItems are %s', item)

        if not item.thumb and self.noImage:
            # no thumb was set yet and no url
            Logger.debug("Setting thumb to %s", item.thumb)
            item.thumb = self.noImage

        if not item.has_media_item_parts():
            item.complete = False
        else:
            item.complete = True
        return item

    # TODO: remove this?
    def DownloadVideoItem(self, item):
        """Downloads an existing MediaItem with more data.

        Arguments:
        item : MediaItem - the MediaItem that should be downloaded.

        Returns:
        The original item with more data added to it's properties.

        Used to download an <item>. If the item is not complete, the self.UpdateVideoItem
        method is called to update the item. The method downloads only the MediaStream
        with the bitrate that was set in the addon settings.

        After downloading the self.downloaded property is set.

        """

        if not item.is_playable():
            Logger.error("Cannot download a folder item.")
            return item

        if item.is_playable():
            if not item.complete:
                Logger.info("Fetching MediaUrl for PlayableItem[%s]", item.type)
                item = self.process_video_item(item)

            if not item.complete or not item.has_media_item_parts():
                Logger.error("Cannot download incomplete item or item without MediaItemParts")
                return item

            i = 1
            bitrate = AddonSettings.get_max_stream_bitrate(self)
            for media_item_part in item.MediaItemParts:
                Logger.info("Trying to download %s", media_item_part)
                stream = media_item_part.get_media_stream_for_bitrate(bitrate)
                download_url = stream.Url
                extension = UriHandler.get_extension_from_url(download_url)
                if len(item.MediaItemParts) > 1:
                    save_file_name = "%s-Part_%s.%s" % (item.name, i, extension)
                else:
                    save_file_name = "%s.%s" % (item.name, extension)
                Logger.debug(save_file_name)

                headers = item.HttpHeaders.copy()
                headers.update(media_item_part.HttpHeaders)

                progress_dialog = XbmcDialogProgressWrapper("Downloading Item", item.name, stream.Url)
                folder_name = XbmcWrapper.show_folder_selection('Select download destination for "%s"' % (save_file_name,))
                UriHandler.download(download_url, save_file_name, folder_name, progress_dialog, proxy=self.proxy,
                                    additional_headers=headers)
                i += 1

            item.downloaded = True

        return item

    #noinspection PyUnusedLocal
    def LogOn(self):
        """Logs on to a website, using an url.

        Returns:
        True if successful.

        First checks if the channel requires log on. If so and it's not already
        logged on, it should handle the log on. That part should be implemented
        by the specific channel.

        More arguments can be passed on, but must be handled by custom code.

        After a successful log on the self.loggedOn property is set to True and
        True is returned.

        """

        if not self.requiresLogon:
            Logger.debug("No login required of %s", self.channelName)
            return True

        if self.loggedOn:
            Logger.info("Already logged in")
            return True

        return False

    def PlayVideoItem(self, item, bitrate=None):
        """Starts the playback of the <item> with the specific <bitrate> in the selected <player>.

        Arguments:
        item    : MediaItem - The item to start playing

        Keyword Arguments:
        bitrate : [opt] integer - The requested bitrate in Kbps or None.
        plugin  : [opt] boolean - Indication whether we are in plugin mode. If True, there
                                  will not actually be playback, rather a tuple with info.

        Returns:
        The updated <item>.

        Starts the playback of the selected MediaItem <item>. Before playback is started
        the item is check for completion (item.complete), if not completed, the self.UpdateVideoItem
        method is called to update the item.

        After updating the requested bitrate playlist is selected, if bitrate was set to None
        the bitrate is retrieved from the addon settings. The playlist is then played using the
        requested player.

        """

        if bitrate is None:
            # use the bitrate from the xbmc settings if bitrate was not specified and the item is MultiBitrate
            bitrate = AddonSettings.get_max_stream_bitrate(self)

        # should we download items?
        Logger.debug("Checking for not streamable parts")
        # We need to substract the download time from processing time
        download_start = datetime.now()
        for part in item.MediaItemParts:
            # TODO: remove the CanStream and Download stuff
            if not part.CanStream:
                stream = part.get_media_stream_for_bitrate(bitrate)
                if not stream.Downloaded:
                    Logger.debug("Downloading not streamable part: %s\nDownloading Stream: %s", part, stream)

                    # we need a unique filename
                    file_name = EncodingHelper.encode_md5(stream.Url)
                    extension = UriHandler.get_extension_from_url(stream.Url)

                    # now we force the busy dialog to close, else we cannot cancel the download
                    # setResolved will not work.
                    LockWithDialog.close_busy_dialog()

                    headers = item.HttpHeaders.copy()
                    headers.update(part.HttpHeaders)

                    Logger.error(headers)
                    stream_filename = "xot.%s.%skbps-%s.%s" % (file_name, stream.Bitrate, item.name, extension)
                    progress_dialog = XbmcDialogProgressWrapper("Downloading Item", item.name, stream.Url)
                    cache_file = UriHandler.download(stream.Url, stream_filename, self.GetDefaultCachePath(),
                                                     progress_dialog.progress_update, proxy=self.proxy,
                                                     additional_headers=headers)

                    if cache_file == "":
                        Logger.error("Cannot download stream %s \nFrom: %s", stream, part)
                        return

                    if cache_file.startswith("\\\\"):
                        cache_file = cache_file.replace("\\", "/")
                        stream.Url = "file:///%s" % (cache_file,)
                    else:
                        stream.Url = "file://%s" % (cache_file,)

                    stream.Downloaded = True

        # We need to substract the download time from processing time
        download_time = datetime.now() - download_start
        download_duration = 1000 * download_time.seconds + download_time.microseconds / 1000

        # Set item as downloaded
        item.downloaded = True

        # get the playlist
        (play_list, srt) = item.get_kodi_play_list(bitrate, update_item_urls=True, proxy=self.proxy)

        # call for statistics with timing
        Statistics.register_playback(self, item, Initializer.StartTime, -download_duration)

        # if the item urls have been updated, don't start playback, but return
        return play_list, srt

    def GetDefaultCachePath(self):
        """ returns the default cache path for this channel

        Could be overridden by a channel.

        """

        return Config.cacheDir

    def GetVerifiableVideoUrl(self, url):
        """Creates an RTMP(E) url that can be verified using an SWF URL.

        Arguments:
        url : string - the URL that should be made verifiable.

        Returns:
        A new URL that includes the self.swfUrl in the form of "url --swfVfy|-W swfUrl".
        If self.swfUrl == "", the original URL is returned.

        """

        if self.swfUrl == "":
            return url

        # Kodi 17.x also accepts an SWF-url as swfvfy option (https://www.ffmpeg.org/ffmpeg-protocols.html#rtmp).
        # This option should be set via the XbmcListItem.setProperty, so within Retrospect via:
        #   part.add_property("swfvfy", self.swfUrl)
        # Or as an URL parameter swfvfy where we add the full URL instead of just 1:
        #   return "%s swfvfy=%s" % (url, self.swfUrl)

        if AddonSettings.is_min_version(17):
            Logger.debug("Using Kodi 17+ RTMP parameters")
            return "%s swfvfy=%s" % (url, self.swfUrl)
        else:
            Logger.debug("Using Legacy (Kodi 16 and older) RTMP parameters")
            return "%s swfurl=%s swfvfy=1" % (url, self.swfUrl)

    def GetImageLocation(self, image):
        """returns the path for a specific image name.

        Arguments:
        image : string - the filename of the requested argument.

        Returns:
        The full local path to the requested image.

        """

        return TextureHandler.instance().get_texture_uri(self, image)

    def _AddDataParsers(self, urls, name=None, preprocessor=None,
                        parser=None, creator=None, updater=None,
                        json=False, matchType=ParserData.MatchStart):
        """ Adds a DataParser to the handlers dictionary for the  given urls

        @param urls:            The URLs that triggers these handlers
        @param preprocessor:    The pre-processor called
        @param parser:          The parser (regex or json)
        @param creator:         The creator called with the results from the parser
        @param updater:         The updater called for updating a item
        @param json:            Indication whether the parsers are JSON (True) or Regex (False)
        @param matchType:       The type of matching to use

        @return: Nothing

        """

        for url in urls:
            self._AddDataParser(url, name, preprocessor, parser, creator, updater, json, matchType=matchType)
        return

    def _GetSetting(self, settingId, valueForNone=None):
        """ Retrieves channel specific settings. Just to prevent us from importing AddonSettings in all channels.

        @param settingId: the channels specific setting
        @return: the settings value from the Add-on using the Kodi settings API

        """

        setting = AddonSettings.get_channel_setting(self, settingId, valueForNone)
        return setting

    # noinspection PyPropertyAccess
    def _AddDataParser(self, url, name=None, preprocessor=None,
                       parser=None, creator=None, updater=None,
                       json=False, matchType=ParserData.MatchStart, requiresLogon=False):
        """ Adds a DataParser to the handlers dictionary

        @param url:             The URL that triggers these handlers
        @param preprocessor:    The pre-processor called
        @param parser:          The parser (regex or json)
        @param creator:         The creator called with the results from the parser
        @param updater:         The updater called for updating a item
        @param json:            Indication whether the parsers are JSON (True) or Regex (False)
        @param matchType:       The type of matching to use
        @param name:            The name of the dataparser
        @param requiresLogon:   Do we need to logon for this?

        @return: Nothing

        """

        data = ParserData(url)
        data.Name = name
        data.PreProcessor = preprocessor
        data.Parser = parser
        data.Creator = creator
        data.Updater = updater
        data.IsJson = json
        data.MatchType = matchType
        data.LogOnRequired = requiresLogon

        if url in self.dataParsers:
            self.dataParsers[url].append(data)
        else:
            self.dataParsers[url] = [data]
        return

    def __GetDataParsers(self, url, ):
        """ Fetches a list of dataparsers that are valid for this URL. The Parsers and Creators can then
        be used to parse the data from the url. The first match is returned.

        If none matches, the self.data_parsers dictionary is checked for generic dataparsers (marked with *).

        If no dataparsers are defined at all, they will be created based on the old regular expressions or the
        JSON queries. The regular expression suppersede the JSON.

        @param url:     The URL to match
        @return:        A list of parsers to use.

        """

        if url == "searchSite" or url == "#searchSite":
            return []

        # For now we need to be backwards compatible:
        if not self.dataParsers:
            # Add the mainlist
            if url == self.mainListUri:
                Logger.debug("No DataParsers found. Adding old Mainlist Creators to DataParsers")
                if self.episodeItemJson is not None:
                    self._AddDataParser(self.mainListUri,
                                        parser=self.episodeItemJson, creator=self.CreateEpisodeItem,
                                        json=True, matchType=ParserData.MatchExact)

                    if self.episodeItemRegex:
                        Logger.warning("Both JSON and Regex parsers available for mainlist, ignoring Regex.")
                else:
                    self._AddDataParser(self.mainListUri,
                                        parser=self.episodeItemRegex, creator=self.CreateEpisodeItem,
                                        matchType=ParserData.MatchExact)
            else:
                # Add the folder and video items
                Logger.debug("No DataParsers found. Adding old FolderList Creators to DataParsers")
                self._AddDataParser("*", preprocessor=self.PreProcessFolderList)

                if self.videoItemJson is not None:
                    # foldder
                    self._AddDataParser("*", parser=self.folderItemJson, creator=self.CreateFolderItem)
                    # video
                    self._AddDataParser("*", parser=self.videoItemJson, creator=self.CreateVideoItem,
                                        updater=self.UpdateVideoItem, json=True)
                    # page
                    self._AddDataParser("*", parser=self.pageNavigationJson, creator=self.CreatePageItem,
                                        json=True)

                    if self.folderItemRegex:
                        Logger.warning("Both JSON and Regex parsers available for folders/videos, ignoring Regex.")
                else:
                    # folder
                    self._AddDataParser("*", parser=self.folderItemRegex, creator=self.CreateFolderItem)
                    # video
                    self._AddDataParser("*", parser=self.videoItemRegex, creator=self.CreateVideoItem,
                                        updater=self.UpdateVideoItem)
                    # page
                    self._AddDataParser("*", parser=self.pageNavigationRegex, creator=self.CreatePageItem)

        # Find the parsers
        # watch = stopwatch.StopWatch('DataParsers', Logger.instance())
        data_parsers = None
        if url.startswith("#"):
            # let's handle the keyword url's
            Logger.trace("Found URL with labeled DataParser keyword [%s]", url)
            if url in self.dataParsers.keys():
                # use the parsers that is associated with No url (the None Parser)
                data_parsers = self.dataParsers[url]
            else:
                Logger.warning("no DataParser was found keyword [%s]. Continuing with other options.", url)
        else:
            # make sure we sort by keylength and then start with the longest one.
            keys = sorted(self.dataParsers.keys(), key=len, reverse=True)
            # watch.lap("DataParsers sorted")

            # filter them in order
            for key in keys:
                # for each key we see if we have filtered results
                data_parsers = filter(lambda p: p.matches(url),
                                     self.dataParsers[key])
                if data_parsers:
                    Logger.trace("Found %s direct DataParsers matches", len(data_parsers))
                    break
            # watch.lap("DataParsers filtered")

        if not data_parsers:
            # Let's use a fallback
            key = "*"
            data_parsers = self.dataParsers.get(key, None)

        # watch.lap("DataParsers processed")

        if not data_parsers:
            Logger.error("No DataParsers found for '%s'", url)
            return []
        else:
            Logger.debug("Found %s DataParsers for '%s'", len(data_parsers), url)
        return data_parsers

    def __str__(self):
        """Returns a string representation of the current channel."""

        if self.channelCode is None:
            return "%s [%s-%s, %s, %s, %s] (Order: %s)" % (
                self.channelName, self.id, self.version, self.language, self.category, self.guid,
                self.sortOrder)
        else:
            return "%s (%s) [%s-%s, %s, %s, %s] (Order: %s)" % (
                self.channelName, self.channelCode, self.id, self.version, self.language,
                self.category, self.guid, self.sortOrder)

    def __eq__(self, other):
        """Compares to channel objects for equality

        Arguments:
        other : Channel - the other channel to compare to

        The comparison is based only on the self.guid of the channels.

        """

        if other is None:
            return False

        return self.guid == other.guid

    def __cmp__(self, other):
        """Compares to channels

        Arguments:
        other : Channel - the other channel to compare to

        Returns:
        The return value is negative if self < other, zero if self == other and strictly positive if self > other

        """

        if other is None:
            return 1

        comp_val = cmp(self.sortOrder, other.sortOrder)
        if comp_val == 0:
            comp_val = cmp(self.channelName, self.channelName)

        return comp_val
